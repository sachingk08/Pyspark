{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyND32yO1PSvKOFfDfSOUyc0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachingk08/Pyspark/blob/main/pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIeSIoSNIz5K",
        "outputId": "d513952d-8e2f-41ff-88d7-fc5cd8262061"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 62 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 50.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=70749c747ad77cb9aadec22befe2c4cd1c26f934348f51ffc5f68aa30514e905\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tlb2w1JYJB5Z"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EDayRWiJPnR"
      },
      "source": [
        "data = [\"Project Gutenberg’s\",\n",
        "\"Alice’s Adventures in Wonderland\",\n",
        "\"Project Gutenberg’s\",\n",
        "\"Adventures in Wonderland\",\n",
        "\"Project Gutenberg’s\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j66KD8QsJS5A",
        "outputId": "c3790bdd-c70b-4f58-eb31-6c90a2e6fbaa"
      },
      "source": [
        "rdd=spark.sparkContext.parallelize(data)\n",
        "for i in rdd.collect():\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project Gutenberg’s\n",
            "Alice’s Adventures in Wonderland\n",
            "Project Gutenberg’s\n",
            "Adventures in Wonderland\n",
            "Project Gutenberg’s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghGrnN7lJdmV",
        "outputId": "b37c9f1b-5762-42bd-f655-ed6edea0b0cd"
      },
      "source": [
        "#Flatmap printing every word in each line\n",
        "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
        "for i in rdd2.collect():\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project\n",
            "Gutenberg’s\n",
            "Alice’s\n",
            "Adventures\n",
            "in\n",
            "Wonderland\n",
            "Project\n",
            "Gutenberg’s\n",
            "Adventures\n",
            "in\n",
            "Wonderland\n",
            "Project\n",
            "Gutenberg’s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SU1D5xmJS14",
        "outputId": "9454f44b-b368-4da1-d27a-aff06d4ada56"
      },
      "source": [
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "a= spark.createDataFrame([\"SAM\",\"JOHN\",\"AND\",\"ROBIN\",\"ANAND\"],\"string\")\\\n",
        ".toDF(\"Name\")\n",
        "print('Hello')\n",
        "a.show()\n",
        "def f(x): print(x)\n",
        "b = a.foreach(f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "+-----+\n",
            "| Name|\n",
            "+-----+\n",
            "|  SAM|\n",
            "| JOHN|\n",
            "|  AND|\n",
            "|ROBIN|\n",
            "|ANAND|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXx_ta-IQXpG"
      },
      "source": [
        "data = [('mike','tisson','M',31797),\n",
        "('roger','fedrik','M',41203),\n",
        "('natasha','Will','F',62817),]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qy8mwgLQwnl",
        "outputId": "1f9b880f-1ae6-4a69-e25c-120363846ae1"
      },
      "source": [
        "columns = [\"firstname\",\"lastname\",\"sex\",\"salary\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "df.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+---+------+\n",
            "|firstname|lastname|sex|salary|\n",
            "+---------+--------+---+------+\n",
            "|     mike|  tisson|  M| 31797|\n",
            "|    roger|  fedrik|  M| 41203|\n",
            "|  natasha|    Will|  F| 62817|\n",
            "+---------+--------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv4s2JjuQ0N9",
        "outputId": "1f6d59c5-ac6e-4649-d387-7097bb35cf2d"
      },
      "source": [
        "rdd=spark.sparkContext.parallelize(data)\n",
        "rdd2=df.rdd.map(lambda x:(x[0]+\",\"+x[1],x[2],x[3]*3))\n",
        "df2=rdd2.toDF([\"name\",\"sex\",\"newSalary\"] )\n",
        "df2.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+---+---------+\n",
            "|        name|sex|newSalary|\n",
            "+------------+---+---------+\n",
            "| mike,tisson|  M|    95391|\n",
            "|roger,fedrik|  M|   123609|\n",
            "|natasha,Will|  F|   188451|\n",
            "+------------+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY6olJONXOWb"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        ".appName('Spark.com')\\\n",
        ".getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKCdhTUQXOTK"
      },
      "source": [
        "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
        "(2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
        "(3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
        "(4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
        "(5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
        "(6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhAX7R1bXOMZ"
      },
      "source": [
        "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
        "\"emp_dept_id\",\"gender\",\"salary\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUD8yx41XOJR",
        "outputId": "68a21556-9b73-44c2-e478-cc345ec58886"
      },
      "source": [
        "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
        "empDF.printSchema()\n",
        "empDF.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- emp_id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- superior_emp_id: long (nullable = true)\n",
            " |-- year_joined: string (nullable = true)\n",
            " |-- emp_dept_id: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li9WY99hhzLK"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import expr\n",
        "spark = SparkSession.builder.appName('Productwithcountry.com').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o08uME-ShzIl"
      },
      "source": [
        "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
        "(\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
        "(\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
        "(\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlbbFlNahzFZ",
        "outputId": "7090e4f8-8a9d-4181-d1ab-3456582e7890"
      },
      "source": [
        "columns= [\"Product\",\"Amount\",\"Country\"]\n",
        "df = spark.createDataFrame(data = data, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Amount: long (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n",
            "+-------+------+-------+\n",
            "|Product|Amount|Country|\n",
            "+-------+------+-------+\n",
            "|Banana |1000  |USA    |\n",
            "|Carrots|1500  |USA    |\n",
            "|Beans  |1600  |USA    |\n",
            "|Orange |2000  |USA    |\n",
            "|Orange |2000  |USA    |\n",
            "|Banana |400   |China  |\n",
            "|Carrots|1200  |China  |\n",
            "|Beans  |1500  |China  |\n",
            "|Orange |4000  |China  |\n",
            "|Banana |2000  |Canada |\n",
            "|Carrots|2000  |Canada |\n",
            "|Beans  |2000  |Mexico |\n",
            "+-------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTeE9xeRhy7D",
        "outputId": "1b4925b9-49ba-4696-a72c-31dc38e5c6bc"
      },
      "source": [
        "pivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
        "pivotDF.printSchema()\n",
        "pivotDF.show(truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Canada: long (nullable = true)\n",
            " |-- China: long (nullable = true)\n",
            " |-- Mexico: long (nullable = true)\n",
            " |-- USA: long (nullable = true)\n",
            "\n",
            "+-------+------+-----+------+----+\n",
            "|Product|Canada|China|Mexico|USA |\n",
            "+-------+------+-----+------+----+\n",
            "|Orange |null  |4000 |null  |4000|\n",
            "|Beans  |null  |1500 |2000  |1600|\n",
            "|Banana |2000  |400  |null  |1000|\n",
            "|Carrots|2000  |1200 |null  |1500|\n",
            "+-------+------+-----+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACwqAa1eh6M5",
        "outputId": "f5481a65-f048-4091-d85e-3c39762b7fdc"
      },
      "source": [
        "pivotDF = df.groupBy(\"Product\",\"Country\") \\\n",
        ".sum(\"Amount\") \\\n",
        ".groupBy(\"Product\") \\\n",
        ".pivot(\"Country\") \\\n",
        ".sum(\"sum(Amount)\")\n",
        "pivotDF.printSchema()\n",
        "pivotDF.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Canada: long (nullable = true)\n",
            " |-- China: long (nullable = true)\n",
            " |-- Mexico: long (nullable = true)\n",
            " |-- USA: long (nullable = true)\n",
            "\n",
            "+-------+------+-----+------+----+\n",
            "|Product|Canada|China|Mexico|USA |\n",
            "+-------+------+-----+------+----+\n",
            "|Orange |null  |4000 |null  |4000|\n",
            "|Beans  |null  |1500 |2000  |1600|\n",
            "|Banana |2000  |400  |null  |1000|\n",
            "|Carrots|2000  |1200 |null  |1500|\n",
            "+-------+------+-----+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHF5h4Vph6Fo",
        "outputId": "056f1dd3-be4f-4c4c-fad1-2229b96e66fb"
      },
      "source": [
        "unpivotExpr = \"stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)\"\n",
        "unPivotDF = pivotDF.select(\"Product\", expr(unpivotExpr)) \\\n",
        ".where(\"Total is not null\")\n",
        "unPivotDF.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----+\n",
            "|Product|Country|Total|\n",
            "+-------+-------+-----+\n",
            "|Orange |China  |4000 |\n",
            "|Beans  |China  |1500 |\n",
            "|Beans  |Mexico |2000 |\n",
            "|Banana |Canada |2000 |\n",
            "|Banana |China  |400  |\n",
            "|Carrots|Canada |2000 |\n",
            "|Carrots|China  |1200 |\n",
            "+-------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8njME6hah5-J"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, substring\n",
        "spark=SparkSession.builder.appName(\"stringoperations\").getOrCreate()\n",
        "data = [(1,\"20200828\"),(2,\"20180525\")]\n",
        "columns=[\"id\",\"date\"]\n",
        "df=spark.createDataFrame(data,columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqXOlPMJi-5e",
        "outputId": "51371e34-8839-4fb6-f3d5-d4d7f4320de1"
      },
      "source": [
        "#Using SQL function substring()\n",
        "df.withColumn('year', substring('date', 1,4))\\\n",
        ".withColumn('month', substring('date', 5,2))\\\n",
        ".withColumn('day', substring('date', 7,2))\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            "\n",
            "+---+--------+\n",
            "|id |date    |\n",
            "+---+--------+\n",
            "|1  |20200828|\n",
            "|2  |20180525|\n",
            "+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70UAmS1oi-0f",
        "outputId": "7d412078-a90e-44d9-8914-787966952ede"
      },
      "source": [
        "#Using select\n",
        "df1=df.select('date', substring('date', 1,4).alias('year'), \\\n",
        "substring('date', 5,2).alias('month'), \\\n",
        "substring('date', 7,2).alias('day'))\n",
        "df1.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----+-----+---+\n",
            "|date    |year|month|day|\n",
            "+--------+----+-----+---+\n",
            "|20200828|2020|08   |28 |\n",
            "|20180525|2018|05   |25 |\n",
            "+--------+----+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DDJ1BPui-xi",
        "outputId": "bca6f161-cc33-4837-b5d4-a82e8ebfcf12"
      },
      "source": [
        "#Using with selectExpr\n",
        "df2=df.selectExpr('date', 'substring(date, 1,4) as year', \\\n",
        "'substring(date, 5,2) as month', \\\n",
        "'substring(date, 7,2) as day')\n",
        "df2.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----+-----+---+\n",
            "|date    |year|month|day|\n",
            "+--------+----+-----+---+\n",
            "|20200828|2020|08   |28 |\n",
            "|20180525|2018|05   |25 |\n",
            "+--------+----+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyzxmmujjGYZ",
        "outputId": "6d5b8367-0664-406e-cef8-9008db2b224d"
      },
      "source": [
        "#Using substr from Column type\n",
        "df3=df.withColumn('year', col('date').substr(1, 4))\\\n",
        ".withColumn('month',col('date').substr(5, 2))\\\n",
        ".withColumn('day', col('date').substr(7, 2))\n",
        "df3.show(truncate=False)\n",
        "df3.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+----+-----+---+\n",
            "|id |date    |year|month|day|\n",
            "+---+--------+----+-----+---+\n",
            "|1  |20200828|2020|08   |28 |\n",
            "|2  |20180525|2018|05   |25 |\n",
            "+---+--------+----+-----+---+\n",
            "\n",
            "+---+--------+----+-----+---+\n",
            "| id|    date|year|month|day|\n",
            "+---+--------+----+-----+---+\n",
            "|  1|20200828|2020|   08| 28|\n",
            "|  2|20180525|2018|   05| 25|\n",
            "+---+--------+----+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE8UBbUa4P4o",
        "outputId": "e1940580-bee6-4817-91b1-1902b70971fe"
      },
      "source": [
        "!pip install pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"CatchSpy\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 61 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 57.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=0003353f251d6fd5bff1a0fdf8dc4356eaffc3ddcbfe8c867e15a45724561584\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BhokMRE4P1v",
        "outputId": "00d4fec9-69cc-4ddf-f876-f99074271b52"
      },
      "source": [
        "spark.sql(\"CREATE TABLE IF NOT EXISTS Invites (SrNo int, Name String, Breed String, Colour String, Location String) Using hive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "za5wxZSg4s4f",
        "outputId": "f58c5874-ae9d-417a-ce44-63144d175b30"
      },
      "source": [
        "spark.sql(\"Truncate Table Invites\")\n",
        "spark.sql(\"Insert into Invites values (1,'Bhalu','Bear','Brown','Away'),(2,'Lucy','Cat','Grey','Home'),(3,'Tiny','Mouse','Grey', 'Home'),(4,'Bruno','Dog','Black', 'Away'),(5,'Moski','Cat','white','Away')\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEthebOv4y6b",
        "outputId": "d5df2218-36ed-4acd-b413-bb430d350b78"
      },
      "source": [
        "print(\"Displaying Records:\")\n",
        "print(spark.sql(\"Select * from Invites\").show())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying Records:\n",
            "+----+-----+-----+------+--------+\n",
            "|SrNo| Name|Breed|Colour|Location|\n",
            "+----+-----+-----+------+--------+\n",
            "|   3| Tiny|Mouse|  Grey|    Home|\n",
            "|   4|Bruno|  Dog| Black|    Away|\n",
            "|   5|Moski|  Cat| white|    Away|\n",
            "|   1|Bhalu| Bear| Brown|    Away|\n",
            "|   2| Lucy|  Cat|  Grey|    Home|\n",
            "+----+-----+-----+------+--------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyb7xqcv49Qy",
        "outputId": "bc76c770-03ca-4e92-aa8b-c6487cdde277"
      },
      "source": [
        "print(\"Total members at the party:\")\n",
        "print(spark.sql(\"Select count(*) from Invites\").show())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total members at the party:\n",
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|       5|\n",
            "+--------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiIa_-XL49Nw",
        "outputId": "3239ec9a-fab6-4396-b8df-99bbb125f674"
      },
      "source": [
        "print(\"Total cats in the party:\")\n",
        "print(spark.sql(\"Select count(*) from Invites where Breed = 'Cat'\").show())\n",
        "print(\"Jerry: Oh I think Lucy is the culprit\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total cats in the party:\n",
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|       2|\n",
            "+--------+\n",
            "\n",
            "None\n",
            "Jerry: Oh I think Lucy is the culprit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1PiIXQf49LH",
        "outputId": "2e3ee10f-90c2-4cd3-f024-bd052bc886db"
      },
      "source": [
        "print(spark.sql(\"Select * from Invites where Breed = 'Cat'\").show())\n",
        "print(\"We are too close to finding them.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+-----+------+--------+\n",
            "|SrNo| Name|Breed|Colour|Location|\n",
            "+----+-----+-----+------+--------+\n",
            "|   5|Moski|  Cat| white|    Away|\n",
            "|   2| Lucy|  Cat|  Grey|    Home|\n",
            "+----+-----+-----+------+--------+\n",
            "\n",
            "None\n",
            "We are too close to finding them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P6dmBxa49I5",
        "outputId": "5fe7f891-accc-4c48-d7bb-6bb04aae7ac1"
      },
      "source": [
        "print(\"Spy Identified:\")\n",
        "print(spark.sql(\"select * from Invites where Breed = 'Cat' and  Colour = 'white'\").show())\n",
        "print(\"Tom He is the spy\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spy Identified:\n",
            "+----+-----+-----+------+--------+\n",
            "|SrNo| Name|Breed|Colour|Location|\n",
            "+----+-----+-----+------+--------+\n",
            "|   5|Moski|  Cat| white|    Away|\n",
            "+----+-----+-----+------+--------+\n",
            "\n",
            "None\n",
            "Tom He is the spy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "HRcxGvat5Gd8",
        "outputId": "3e1fee4a-7562-423a-ff6b-ede69630d5cd"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "conf = SparkConf().setAppName(\"CricketMatch\")\n",
        "sc = SparkContext(conf=conf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-fa9dae59471b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CricketMatch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    345\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 347\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    348\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=CatchSpy, master=local[*]) created by getOrCreate at <ipython-input-2-56e8bce9989f>:4 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSW8cqpE5GbX"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"CricketMatch\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "-TlxH5l45GYk",
        "outputId": "943b604f-4fc9-430c-96d4-cd399c6c44bb"
      },
      "source": [
        "#Drop tables \n",
        "spark.sql(\"Drop table TeamTom\")\n",
        "spark.sql(\"Drop table TeamJerry\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1083edfa45c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Drop tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Drop table TeamTom\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Drop table TeamJerry\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found for 'DROP TABLE': TeamTom; line 1 pos 0;\n'DropTable false, false\n+- 'UnresolvedTableOrView [TeamTom], DROP TABLE, true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mjwj-r1k5GVZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC4RFhK85GSG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}